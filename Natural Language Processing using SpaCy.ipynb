{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load spacy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity:  Barack Obama\n",
      "Entity Type: PERSON | People, including fictional\n",
      "Start Offset of the Entity:  0\n",
      "End Offset of the Entity:  12\n",
      "--\n",
      "Entity:  44th\n",
      "Entity Type: ORDINAL | \"first\", \"second\", etc.\n",
      "Start Offset of the Entity:  21\n",
      "End Offset of the Entity:  25\n",
      "--\n",
      "Entity:  the United States of America\n",
      "Entity Type: GPE | Countries, cities, states\n",
      "Start Offset of the Entity:  39\n",
      "End Offset of the Entity:  67\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = spacy.load(\"en\")\n",
    "#nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text: The original word text.\n",
    "Lemma: The base form of the word.\n",
    "POS: The simple part-of-speech tag.\n",
    "Tag: The detailed part-of-speech tag.\n",
    "Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "Shape: The word shape â€“ capitalisation, punctuation, digits.\n",
    "is alpha: Is the token an alpha character?\n",
    "is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tolen: Matthew, lemma: Matthew, pos is: PROPN, tag is NNP, entity is: PERSON, dependency is: compound, is shape: Xxxxx, is alpha: True,is stop: False, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: Honnibal, lemma: Honnibal, pos is: PROPN, tag is NNP, entity is: PERSON, dependency is: nsubj, is shape: Xxxxx, is alpha: True,is stop: False, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: who, lemma: who, pos is: PRON, tag is WP, entity is: , dependency is: nsubjpass, is shape: xxx, is alpha: True,is stop: True, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: can, lemma: can, pos is: VERB, tag is MD, entity is: , dependency is: aux, is shape: xxx, is alpha: True,is stop: True, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: be, lemma: be, pos is: VERB, tag is VB, entity is: , dependency is: auxpass, is shape: xx, is alpha: True,is stop: True, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: contacted, lemma: contact, pos is: VERB, tag is VBN, entity is: , dependency is: relcl, is shape: xxxx, is alpha: True,is stop: False, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: at, lemma: at, pos is: ADP, tag is IN, entity is: , dependency is: prep, is shape: xx, is alpha: True,is stop: True, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: honnibal@spacy.io, lemma: honnibal@spacy.io, pos is: PROPN, tag is NNP, entity is: , dependency is: pobj, is shape: xxxx@xxxx.xx, is alpha: False,is stop: False, is like url: False, is like email: True, is like num: False\n",
      "--------------------------------------\n",
      "tolen: ,, lemma: ,, pos is: PUNCT, tag is ,, entity is: , dependency is: punct, is shape: ,, is alpha: False,is stop: False, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: initially, lemma: initially, pos is: ADV, tag is RB, entity is: , dependency is: advmod, is shape: xxxx, is alpha: True,is stop: False, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: released, lemma: release, pos is: VERB, tag is VBD, entity is: , dependency is: ROOT, is shape: xxxx, is alpha: True,is stop: False, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: SpaCy, lemma: SpaCy, pos is: PROPN, tag is NNP, entity is: DATE, dependency is: dobj, is shape: XxxXx, is alpha: True,is stop: False, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: in, lemma: in, pos is: ADP, tag is IN, entity is: , dependency is: prep, is shape: xx, is alpha: True,is stop: True, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: 2015, lemma: 2015, pos is: NUM, tag is CD, entity is: DATE, dependency is: pobj, is shape: dddd, is alpha: False,is stop: False, is like url: False, is like email: False, is like num: True\n",
      "--------------------------------------\n",
      "tolen: which, lemma: which, pos is: DET, tag is WDT, entity is: , dependency is: nsubjpass, is shape: xxxx, is alpha: True,is stop: True, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: can, lemma: can, pos is: VERB, tag is MD, entity is: , dependency is: aux, is shape: xxx, is alpha: True,is stop: True, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: be, lemma: be, pos is: VERB, tag is VB, entity is: , dependency is: auxpass, is shape: xx, is alpha: True,is stop: True, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: forked, lemma: fork, pos is: VERB, tag is VBN, entity is: , dependency is: relcl, is shape: xxxx, is alpha: True,is stop: False, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: from, lemma: from, pos is: ADP, tag is IN, entity is: , dependency is: prep, is shape: xxxx, is alpha: True,is stop: True, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: the, lemma: the, pos is: DET, tag is DT, entity is: , dependency is: det, is shape: xxx, is alpha: True,is stop: True, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: following, lemma: follow, pos is: VERB, tag is VBG, entity is: , dependency is: amod, is shape: xxxx, is alpha: True,is stop: False, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: repo, lemma: repo, pos is: NOUN, tag is NN, entity is: , dependency is: pobj, is shape: xxxx, is alpha: True,is stop: False, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: :, lemma: :, pos is: PUNCT, tag is :, entity is: , dependency is: punct, is shape: :, is alpha: False,is stop: False, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: https://github.com/explosion/spaCy, lemma: https://github.com/explosion/spacy, pos is: NUM, tag is CD, entity is: , dependency is: appos, is shape: xxxx://xxxx.xxx/xxxx/xxxXx, is alpha: False,is stop: False, is like url: True, is like email: False, is like num: False\n",
      "--------------------------------------\n",
      "tolen: ., lemma: ., pos is: PUNCT, tag is ., entity is: , dependency is: punct, is shape: ., is alpha: False,is stop: False, is like url: False, is like email: False, is like num: False\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_attributes(doc):\n",
    "    for token in doc:\n",
    "        print(\"tolen: %s, lemma: %s, pos is: %s, tag is %s, entity is: %s, \"\n",
    "              \"dependency is: %s, is shape: %s, is alpha: %s,is stop: %s, is like url: %s, \"\n",
    "              \"is like email: %s, is like num: %s\" %\n",
    "              (token.text, token.lemma_, token.pos_, token.tag_, token.ent_type_,\n",
    "               token.dep_, token.shape_, token.is_alpha, token.is_stop,token.like_url, \n",
    "               token.like_email,token.like_num))\n",
    "        print('--------------------------------------')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text =nlp(\"Matthew Honnibal who can be contacted at honnibal@spacy.io, initially released SpaCy in 2015 which can \" \\\n",
    "           \"be forked from the following repo: https://github.com/explosion/spaCy.\") \n",
    "    get_attributes(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity is: Barack Obama, entity label is: PERSON, start offset of the entity is: 0, end offset of the entity is: 12\n",
      "----------------------\n",
      "entity is: 44th, entity label is: ORDINAL, start offset of the entity is: 21, end offset of the entity is: 25\n",
      "----------------------\n",
      "entity is: the United States of America, entity label is: GPE, start offset of the entity is: 39, end offset of the entity is: 67\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "def get_entity(doc):\n",
    "    for token in doc.ents:\n",
    "        print('entity is: %s, entity label is: %s, start offset of the entity is: %s, end offset of the entity is: %s'\n",
    "             %(token.text, token.label_, token.start_char, token.end_char))\n",
    "        print('----------------------')\n",
    "if __name__ == '__main__':\n",
    "    text = nlp('Barack Obama was the 44th president of the United States of America.')\n",
    "    get_entity(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog eats 0.08646662\n",
      "dog banana 0.47929144\n",
      "dog . -0.13729073\n",
      "eats dog 0.08646662\n",
      "eats eats 1.0\n",
      "eats banana 0.057885718\n",
      "eats . -0.022328515\n",
      "banana dog 0.47929144\n",
      "banana eats 0.057885718\n",
      "banana banana 1.0\n",
      "banana . 0.012143209\n",
      ". dog -0.13729073\n",
      ". eats -0.022328515\n",
      ". banana 0.012143209\n",
      ". . 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "//anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "//anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "//anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "//anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "//anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "//anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "//anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "//anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "//anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "//anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "//anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "def cal_similarity(doc1,doc2):\n",
    "    for token1 in doc1:\n",
    "        for token2 in doc2:\n",
    "            print(token1, token2, token1.similarity(token2))\n",
    "if __name__ == '__main__':\n",
    "    doc1 = nlp(u\"dog eats banana.\")\n",
    "    doc2 = nlp(u\"dog eats banana.\")\n",
    "    cal_similarity(doc1,doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0 is: SpaCy is an open-source software library that is published and distributed under MIT license, and is \n",
      "            developed for performing simple to advanced Natural Language Processing (N.L.P) tasks such as tokenization,\n",
      "             part-of-speech tagging, named entity recognition, text classification, and calculating semantic \n",
      "             similarities between text, lemmatization, dependency parsing, among others.\n",
      "start from 0, end at 68\n",
      "----------------------------\n",
      "sentence 1 is: It was originally developed \n",
      "             by Matthew Honnibal and is still maintained by him along with Ines Montani.\n",
      "start from 68, end at 87\n",
      "----------------------------\n",
      "sentence 2 is: SpaCy is written using Python \n",
      "             and Cython.\n",
      "start from 87, end at 96\n",
      "----------------------------\n",
      "sentence 3 is: Under the hood it uses Thinc, a machine learning library written and maintained by the \n",
      "             creators of SpaCy.\n",
      "start from 96, end at 117\n",
      "----------------------------\n",
      "sentence 4 is: Thinc is built ground up for efficient CPU usage and optimized for developing code and \n",
      "             models related to N.L.P and deep learning on text.\n",
      "start from 117, end at 143\n",
      "----------------------------\n",
      "sentence 5 is: At the time of writing of this book SpaCy offers the \n",
      "             fastest syntactic parser in the world.\n",
      "start from 143, end at 162\n",
      "----------------------------\n",
      "sentence 6 is: It also offers state-of-the-art statistical and neural network \n",
      "             models for different languages such as English, German, Spanish, Portuguese, French, Italian and Dutch. \n",
      "             \n",
      "start from 162, end at 198\n",
      "----------------------------\n",
      "sentence 7 is: Due to its large open source community, there is a continuous ongoing development of statistical models on \n",
      "             various other languages including low-resource languages such as Hindi, Marathi, and many more.\n",
      "start from 198, end at 235\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "def split_sentence(doc):\n",
    "    for i, sent in enumerate(doc.sents):\n",
    "        print('sentence %s is: %s'%(i, sent.text))\n",
    "        print('start from %s, end at %s' %(sent.start, sent.end))\n",
    "        print('----------------------------')\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    doc = nlp('''SpaCy is an open-source software library that is published and distributed under MIT license, and is \n",
    "            developed for performing simple to advanced Natural Language Processing (N.L.P) tasks such as tokenization,\n",
    "             part-of-speech tagging, named entity recognition, text classification, and calculating semantic \n",
    "             similarities between text, lemmatization, dependency parsing, among others. It was originally developed \n",
    "             by Matthew Honnibal and is still maintained by him along with Ines Montani. SpaCy is written using Python \n",
    "             and Cython. Under the hood it uses Thinc, a machine learning library written and maintained by the \n",
    "             creators of SpaCy. Thinc is built ground up for efficient CPU usage and optimized for developing code and \n",
    "             models related to N.L.P and deep learning on text. At the time of writing of this book SpaCy offers the \n",
    "             fastest syntactic parser in the world. It also offers state-of-the-art statistical and neural network \n",
    "             models for different languages such as English, German, Spanish, Portuguese, French, Italian and Dutch. \n",
    "             Due to its large open source community, there is a continuous ongoing development of statistical models on \n",
    "             various other languages including low-resource languages such as Hindi, Marathi, and many more.''')\n",
    "    split_sentence(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun Phrase Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun Phrase:  Barack Obama\n",
      "Start Offset of the Noun Phrase:  0\n",
      "End Offset of the Noun Phrase:  12\n",
      "--------------------------\n",
      "Noun Phrase:  the 44th president\n",
      "Start Offset of the Noun Phrase:  17\n",
      "End Offset of the Noun Phrase:  35\n",
      "--------------------------\n",
      "Noun Phrase:  the United States\n",
      "Start Offset of the Noun Phrase:  39\n",
      "End Offset of the Noun Phrase:  56\n",
      "--------------------------\n",
      "Noun Phrase:  America\n",
      "Start Offset of the Noun Phrase:  60\n",
      "End Offset of the Noun Phrase:  67\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_phrases(doc):\n",
    "    for phrase in doc.noun_chunks:\n",
    "        print(\"Noun Phrase: \", phrase.text)\n",
    "        print(\"Start Offset of the Noun Phrase: \", phrase.start_char)\n",
    "        print(\"End Offset of the Noun Phrase: \", phrase.end_char)\n",
    "        print(\"--------------------------\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    doc = nlp(\"Barack Obama was the 44th president of the United States of America.\")\n",
    "    get_phrases(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 text       root  root.text root.dep root.head.text\n",
      "0        Barack Obama      Obama      Obama    nsubj            was\n",
      "1  the 44th president  president  president     attr            was\n",
      "2   the United States     States     States     pobj             of\n",
      "3             America    America    America     pobj             of\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def infor_chunk(doc):\n",
    "    noun_chunks_df = pd.DataFrame()\n",
    "    for i,phrase in enumerate(doc.noun_chunks):\n",
    "        noun_chunks_df.loc[i,'text'] = phrase.text\n",
    "        noun_chunks_df.loc[i,'root'] = phrase.root\n",
    "        noun_chunks_df.loc[i,'root.text'] = phrase.root.text\n",
    "        noun_chunks_df.loc[i,'root.dep'] = phrase.root.dep_\n",
    "        noun_chunks_df.loc[i, 'root.head.text'] = phrase.root.head.text\n",
    "    \n",
    "    print(noun_chunks_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    doc = nlp(\"Barack Obama was the 44th president of the United States of America.\")\n",
    "    infor_chunk(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
